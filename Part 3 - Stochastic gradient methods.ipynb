{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.opt_types import *\n",
    "from lib.utils import *\n",
    "from lib.part_one import f, f_full_data, x_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stochastic gradient methods - 30 Points\n",
    "\n",
    "\n",
    "In this problem, you will implement three different versions of stochastic gradient descent to solve the logistic regression problem on the entire NBA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the stochastic gradient descent methods, we recast our estimation problem [3](#mjx-eqn-eq3) as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{6}\n",
    "f(\\mathbf{x}) = \\frac{1}{n}\\sum_{i=1}^n\\bigg\\{\\underbrace{\\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x})) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2}_{f_i(\\mathbf{x})}\\bigg\\},\n",
    "\\end{equation}\n",
    "\n",
    "where we for notational convenience suppress the dependency on $\\mu$.\n",
    "As we saw in Problem 1, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f_i(\\mathbf{x}) = -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\end{equation}\n",
    "\n",
    "The objective function can be written as a sum of $n$ terms. We augment the `Function` type introduced in previous notebooks to include the following attributes:\n",
    "\n",
    "- To access the gradient of the `j`-th term at a point `x` you can write `f.i_grad(j, x)`.\n",
    "- The number of terms $n$ is stored in the attribute `f.n`. \n",
    "\n",
    "Consider the following stochastic gradient update: At the iteration $k$, pick $i_k\\in\\{1, \\ldots, n\\}$ uniformly at random and define\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{k+1} :=  \\mathbf{x}^k - \\alpha_k\\nabla{f}_{i_k}(\\mathbf{x}^k).\\tag{SGD}\n",
    "\\end{equation}\n",
    "\n",
    "__(a)__ (2 points) Show that $\\nabla f_{i_k}(\\mathbf{x})$ is an unbiased estimation of $\\nabla f(\\mathbf{x})$. Explain why $\\nabla f_{i_k}$ is Lipschitz continuous with $L(f_{i_k}) = \\|\\mathbf{a}_{i_k}\\|^2+ \\mu $. \n",
    "\n",
    "__Hint__: Recall how we computed $L$ in Problem 1. In the following, we will set $L_{\\max} = \\max_{i\\in\\{1, \\ldots, n\\}}L(f_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : To show that $\\nabla f_{i_k}(\\mathbf{x})$ is an unbiased estimation of $\\nabla f(\\mathbf{x})$, we need to demonstrate that the expected value of $\\nabla f_{i_k}(\\mathbf{x})$ is equal to $\\nabla f(\\mathbf{x})$. The gradient of the objective function $f(\\mathbf{x})$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Since $i_k$ is chosen uniformly at random from $\\{1, \\ldots, n\\}$, the expected value of $\\nabla f_{i_k}(\\mathbf{x})$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\nabla f_{i_k}(\\mathbf{x})] = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\nabla f_{i_k}(\\mathbf{x})] = \\nabla f(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "To show that $\\nabla f_{i_k}$ is Lipschitz continuous with $L(f_{i_k}) = \\|\\mathbf{a}_{i_k}\\|^2 + \\mu$, we need to demonstrate that the gradient $\\nabla f_{i_k}(\\mathbf{x})$ satisfies the Lipschitz condition. \n",
    "\n",
    "We take the gradient of $f_i(\\mathbf{x})$ and the Hessian as we already did in part 1:\n",
    "\n",
    "$$\n",
    "\\nabla f_i(\\mathbf{x}) = -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla^2 f_i(\\mathbf{x}) = \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "The maximum eigenvalue of $\\nabla^2 f_i(\\mathbf{x})$ provides the Lipschitz constant for $\\nabla f_i(\\mathbf{x})$, which we already did in Part1, questions j. Therefore, $\\nabla f_{i_k}$ is Lipschitz continuous with $L(f_{i_k}) = \\|\\mathbf{a}_{i_k}\\|^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 points) \n",
    "We can use the standard stochastic gradient descent method [SGD](#mjx-eqn-eqSGD) to solve [6](#mjx-eqn-eq6). \n",
    "Implement `SGD` by completing the following cells with $\\alpha_k =\\frac{0.01}{k}$.\n",
    "\n",
    "**Hint**: For some `N`, the function `np.random.choice(N)` generates a random integer in $\\{0, \\dots, N-1 \\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SGD_state(OptState):\n",
    "    x_k: Vector\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_update(f, state):\n",
    "    x_k, k = state\n",
    "    \n",
    "    i_k = np.random.choice(f.n)\n",
    "    \n",
    "    next_x_k = x_k - (0.01/k)*f.i_grad(i_k, x_k)\n",
    "    \n",
    "    return SGD_state(next_x_k, k+1)\n",
    "\n",
    "def SGD_initialize(f, x_zero):\n",
    "    return SGD_state(x_zero, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = OptAlgorithm(name=\"SGD\", init_state=SGD_initialize, state_update=SGD_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (6 points) \n",
    "Consider the following stochastic averaging gradient method [SAG](#mjx-eqn-eqSAG) to solve [6](#mjx-eqn-eq6):\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{pick } i_k\\in\\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\mathbf{x}^{k+1} := \\mathbf{x}^k - \\frac{\\alpha_k}{n}\\sum_{i=1}^n\\mathbf{v}_i^k,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "\\mathbf{v}_i^k =\n",
    "\\begin{cases}\n",
    "\\nabla f_i(\\mathbf{x}^k) &\\text{if}\\, i = i_k,\\\\\n",
    "\\mathbf{v}_i^{k-1} &\\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Implement `SAG` by completing the following cells using the step-size \n",
    "$\\alpha_k=\\frac{0.01}{L_{\\max}}$ and $\\mathbf{v}^0=\\mathbf{0}$. (Note that you can access $L_{\\max}$ by writing `f.L_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAG_state(OptState):\n",
    "    x_k: Vector\n",
    "    v_k: List[Vector]\n",
    "    alpha_k: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAG_update(f, state):\n",
    "    x_k, v_k, alpha_k = state\n",
    "\n",
    "    i_k = np.random.choice(f.n)\n",
    "    v_k[i_k] = f.i_grad(i_k, x_k)\n",
    "    avg_grad = np.mean(v_k, axis=0)\n",
    "    next_x_k = x_k - alpha_k * avg_grad\n",
    "\n",
    "    return SAG_state(next_x_k, v_k, alpha_k)\n",
    "\n",
    "def SAG_initialize(f, x_zero):\n",
    "    v_zero = [np.zeros_like(x_zero) for _ in range(f.n)]\n",
    "    alpha_k = 0.01 / f.L_max\n",
    "    return SAG_state(x_zero, v_zero, alpha_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAG = OptAlgorithm(name=\"SAG\", init_state=SAG_initialize, state_update=SAG_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs([SAG], f, x_zero, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (6 points) \n",
    "We can improve the convergence rate of SGD by periodically computing the full gradient. `SVRG` uses the following variance reduction scheme:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad \\mathbf{z} = \\mathbf{x}^k\\\\\n",
    "\\quad \\tilde{\\mathbf{v}} = \\nabla f(\\mathbf{x}^k)\\\\\n",
    "\\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\mathbf{d}^k = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}(\\mathbf{z}) + \\tilde{\\mathbf{v}}\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma\\mathbf{d}^k\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Implement `SVRG` by completing the following cells and fixing the following constant:\n",
    "$\\gamma = 1/L_{\\max}$ and $q = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SVRG_state(OptState):\n",
    "    x_k: Vector \n",
    "    z: Vector\n",
    "    tilde_v: Vector\n",
    "    q: int\n",
    "    gamma: float\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVRG_update(f, state):\n",
    "    x_k, z, tilde_v, q, gamma, k = state\n",
    "\n",
    "    if k % q == 0:\n",
    "        z = x_k\n",
    "        tilde_v = f.grad(x_k)\n",
    "\n",
    "    i_k = np.random.choice(f.n)\n",
    "    d_k = f.i_grad(i_k, x_k) - f.i_grad(i_k, z) + tilde_v\n",
    "    next_x_k = x_k - gamma * d_k\n",
    "\n",
    "    return SVRG_state(next_x_k, z, tilde_v, q, gamma, k + 1)\n",
    "\n",
    "def SVRG_initialize(f, x_zero):\n",
    "    return SVRG_state(x_zero, x_zero, np.zeros_like(x_zero), 100, 1/f.L_max, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVRG = OptAlgorithm(name=\"SVRG\", init_state=SVRG_initialize, state_update=SVRG_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (8 points) Another variance reduction method is `SARAH`. The scheme can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{x}}^k = {\\mathbf{z}}\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f({\\mathbf{z}})\\\\\n",
    "\\quad t \\in \\{0, \\ldots, q-1\\} \\text{ uniformly at random}\\\\\n",
    "\\text{if } k = t \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{z}} = {\\mathbf{x}}^{k}\\\\\n",
    "\\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "{\\mathbf{v}}^{k+1} = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}({\\mathbf{x}^{k-1}}) + {\\mathbf{v}}^{k}\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma {\\mathbf{v}}^{k+1}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Implement `SARAH` by completing the following cells. Pick `q=100` and $\\gamma = \\frac{1}{L_{\\max}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SARAH_state(OptState):\n",
    "    x_k: Vector \n",
    "    prev_x_k: Vector\n",
    "    v_k: Vector\n",
    "    z: Vector\n",
    "    q: int\n",
    "    t: int\n",
    "    gamma: float\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARAH_update(f, state):\n",
    "    x_k, prev_x_k, v_k, z, q, t, gamma, k = state\n",
    "\n",
    "    if k % q == 0:\n",
    "        x_k = z\n",
    "        v_k = f.grad(z)\n",
    "        t = np.random.choice(q)\n",
    "\n",
    "    if k % q == t:\n",
    "        z = x_k\n",
    "\n",
    "    i_k = np.random.choice(f.n)\n",
    "    v_k_next = f.i_grad(i_k, x_k) - f.i_grad(i_k, prev_x_k) + v_k\n",
    "    next_x_k = x_k - gamma * v_k_next\n",
    "\n",
    "    return SARAH_state(next_x_k, x_k, v_k_next, z, q, t, gamma, k + 1)\n",
    "\n",
    "def SARAH_initialize(f, x_zero):\n",
    "    return SARAH_state(x_zero, x_zero, np.zeros_like(x_zero), x_zero, 100, np.random.choice(100), 1 / f.L_max, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARAH = OptAlgorithm(name=\"SARAH\", init_state=SARAH_initialize, state_update=SARAH_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(f)__ (6 points) Another variance reduction method is `Spider`. The scheme can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f({\\mathbf{x}}^k)\\\\\n",
    "\\text{if } k \\neq 0 \\text{ mod } q:\\\\\n",
    "\\quad \\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}({\\mathbf{x}}^{k-1}) + {\\mathbf{v}}^{k-1}\\\\\n",
    "\\eta := \\min \\big(\\frac{1}{\\left \\| {\\mathbf{v}}^{k} \\right \\|_2}, \\frac{1}{\\epsilon}\\big)\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma \\eta {\\mathbf{v}}^{k}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Implement `Spider` by completing the following cells. Pick `q=100`, $\\epsilon = 0.05$ and $\\gamma = \\frac{1}{L_{\\max}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Spider_state(OptState):\n",
    "    x_k: Vector \n",
    "    prev_x_k: Vector\n",
    "    prev_v_k: Vector\n",
    "    q: int\n",
    "    gamma: float\n",
    "    k: int\n",
    "    epsilon: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def Spider_update(f, state):\n",
    "    x_k, prev_x_k, prev_v_k , gamma, q, epsilon, k = state\n",
    "\n",
    "    if k % q == 0:\n",
    "        v_k = f.grad(x_k)\n",
    "    else:\n",
    "        i_k = np.random.randint(f.n)\n",
    "        v_k = f.i_grad(i_k, x_k) - f.i_grad(i_k, prev_x_k) + prev_v_k\n",
    "    \n",
    "    eta = min(1 / norm(v_k, 2), 1 / epsilon)\n",
    "    next_x_k = x_k - gamma * eta * v_k\n",
    "\n",
    "    return Spider_state(x_k=next_x_k, prev_x_k=x_k, prev_v_k=v_k, q=q, gamma=gamma, k=k+1, epsilon=epsilon)\n",
    "\n",
    "def Spider_initialize(f, x_zero):\n",
    "    return Spider_state(x_k=x_zero, prev_x_k=x_zero, prev_v_k=f.grad(x_zero), q=100, gamma=1 / f.L_max, k=0, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spider = OptAlgorithm(name=\"Spider\", init_state=Spider_initialize, state_update=Spider_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs([SVRG, SARAH, Spider], f_full_data, x_zero, 50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
