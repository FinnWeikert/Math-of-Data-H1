{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we consider a binary classification task that we will model using logistic regression. Your goal will be to find a classifier using first-order methods and accelerated gradient descent methods. The first part will consist of more theoretical questions, and the second one will ask you to implement these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  ℹ️ <strong>Information on group based work:</strong>\n",
    "</div>\n",
    "\n",
    "- You are to deliver only 1 notebook per group.\n",
    "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
    "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
    "  ⚠️ Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person 1 **Name**: || Person 1 **SCIPER**:\n",
    "\n",
    "\n",
    "Person 2 **Name**: || Person 2 **SCIPER**:\n",
    "\n",
    "\n",
    "Person 3 **Name**: || Person 3 **SCIPER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression - 15 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classic approach to _binary classification_. Before we dive in, let us first define the standard logistic function $\\sigma$ on which most of what follows is built:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sigma : x \\mapsto \\frac{1}{1 + \\exp{(-x)}}.\n",
    "\\end{equation*}\n",
    "\n",
    "In logistic regression, we model the _conditional probability_ of observing a class label $b$ given a set of features $\\mathbf{a}$. More formally, if we observe $n$ independent samples\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{a}_i\\in\\mathbb{R}^p$ and $b_i\\in\\{-1, +1\\}$ is the class label, we _assume_ that $b_i$ given $\\mathbf{a}_i$ is a symmetric Bernouilli random variable with parameter $\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)$, for some unknown $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$. In other words, we assume that there exists an $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = 1 \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural) \\quad \\text{ and } \\quad \\mathbb{P}(b_i = -1 \\mid \\mathbf{a}_i) = 1 - \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)=  \\sigma( - \\mathbf{a}_i^T\\mathbf{x}^\\natural).\n",
    "\\end{equation*}\n",
    "\n",
    "This is our statistical model. It can be written in a more compact form as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma(j \\cdot \\mathbf{a}_i^T\\mathbf{x}^\\natural), \\quad j \\in \\{+1, -1\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal now is to determine the unknown $\\mathbf{x}^\\natural$ by constructing an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ (1 point) We are provided with a set of $n$ independent observations. Show that the negative log-likelihood $f$ can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tf(\\mathbf{x}) = -\\log(\\mathbb{P}(b_1, \\dots, b_n | a_1, \\dots, a_n)) & = \\sum_{i=1}^n  \\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x})).\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : We start by considering a single observation, that is the likelihood of observing the class labels $b_i$ given the features $\\mathbf{a}_i$. The probability of observing $b_i$ given $\\mathbf{a}_i$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(b_i \\mid \\mathbf{a}_i) = \\sigma(b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}^\\natural) = \\frac{1}{1 + \\exp(- b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}^\\natural)}\n",
    "$$\n",
    "\n",
    "Since the observations are independent, the joint likelihood of observing all $n$ samples is the product of the individual likelihoods:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(b_1, \\dots, b_n \\mid \\mathbf{a}_1, \\dots, \\mathbf{a}_n) = \\prod_{i=1}^n \\mathbb{P}(b_i \\mid \\mathbf{a}_i) = \\prod_{i=1}^n \\frac{1}{1 + \\exp(- b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}^\\natural)}.\n",
    "$$\n",
    "\n",
    "Then, by applying the rules of calculus we get the following:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = -\\log\\left( \\prod_{i=1}^n \\frac{1}{1 + \\exp(- b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}^\\natural)} \\right) = -\\sum_{i=1}^n \\log\\left( \\frac{1}{1 + \\exp(- b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}^\\natural)} \\right) = \\sum_{i=1}^n \\log(1 + \\exp(- b_i \\cdot \\mathbf{a}_i^T \\mathbf{x}^\\natural)).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 point) Show that the function $u \\mapsto \\log(1 + \\exp(-u))$ is convex. Deduce that $f$ is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def g(u):\n",
    "    return np.log(1 + np.exp(-u))\n",
    "\n",
    "# Generate values for u\n",
    "u_values = np.linspace(-5, 5, 400)\n",
    "g_values = g(u_values)\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(u_values, g_values, label=r'$\\log(1 + \\exp(-u))$', color='blue')\n",
    "plt.title(r'Graph of $g(u) = \\log(1 + \\exp(-u))$')\n",
    "plt.xlabel('u')\n",
    "plt.ylabel('g(u)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the drawing, we can directly conclude that this is a convex function. If we take the function $f$, each term in the summation, $\\log(1 + \\exp(- b_i \\mathbf{a}_i^T \\mathbf{x}))$, is a composition of the convex function $g(u) = \\log(1 + \\exp(-u))$ with the linear function $u = b_i \\mathbf{a}_i^T \\mathbf{x}$. Since the composition of a convex function with a linear function is convex, each term in the summation is convex. The sum of convex functions is also convex. Therefore, $f(\\mathbf{x})$ is convex.\n",
    "\n",
    "For a more formal demonstration:\n",
    "\n",
    "The second derivative of $\\log(1 + \\exp(-u))$ gives $\\frac{\\exp(-u)}{(1 + \\exp(-u))^2}$ and the second derivative is positive so, by the definition given by the course (lecture 3, page 28), the function $\\log(1 + \\exp(-u))$ is convex. It follows that $\\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x}))$ is convex and knowing that the sum of convex functions gives a convex functions, we proved that $f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator, which is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^\\star_{ML} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}),\n",
    "\\end{equation*}\n",
    "\n",
    "is a global minimum so it can serve as an estimator of $\\mathbf{x}^\\natural$. But, does the minimum always exist? We will ponder this question in the following three points.\n",
    "\n",
    "__(c)__ (1 point) Explain the difference between infima and minima.  Give an example of a convex function, defined over $\\mathbb{R}$, that does not attain its infimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : The infimum of a function over a domain is the greatest lower bound of the function's values over that domain (it may or may not be an element of the set itself), while the minimum is the smallest value the function actually attains within that domain, if it exists.\n",
    "\n",
    "Consider the function $f(x) = \\exp(x)$, which is defined over $\\mathbb{R}$. This function is convex because its second derivative is positive for all $x \\in \\mathbb{R}$: $f''(x) = \\exp(x) > 0 $.\n",
    "\n",
    "However, the function does not attain its infimum over $\\mathbb{R}$. The infimum of $f(x)$ over $\\mathbb{R}$ is 0, because $\\exp(x) > 0$ for all $x \\in \\mathbb{R}$ and $\\exp(x)$ can get arbitrarily close to 0 as $x \\to -\\infty$. But there is no $x \\in \\mathbb{R}$ such that $\\exp(x) = 0$. Therefore, the infimum is not in the set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (1 point) Let us assume that there exists $\\mathbf{x}_0 \\in \\mathbb{R}^p$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0.\n",
    "\\end{equation*}\n",
    "\n",
    "This is called _complete separation_ in the literature. Can you think of a geometric reason why this name is appropriate? Think of a 2D example where this can happen (i.e $p=2$) and describe why _complete separation_ is an appropriate name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : The term \"**complete separation**\" is appropriate because it describes a situation where the data points belonging to different classes can be perfectly separated by a hyperplane. In other words, there exists a vector $\\mathbf{x}_0$ such that the dot product $b_i \\mathbf{a}_i^T \\mathbf{x}_0$ is positive for all data points $(\\mathbf{a}_i, b_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you should see that it is likely that our data satisfies the complete separation assumption. Unfortunately, as you will show in the following question, this can become an obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (1 point) In a _complete separation_ setting, i.e, there exists $\\mathbf{x}_0$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0,\n",
    "\\end{equation*}\n",
    "\n",
    "prove that the function $f$ does not attain its minimum. \n",
    "\n",
    "__Hint__: If the function did have a minimum, would it be above, below or equal to zero? Then think of how $f(2 \\mathbf{x}_0)$ compares with $f(\\mathbf{x}_0)$, how about $f(\\alpha \\mathbf{x}_0)$ for $\\alpha \\rightarrow + \\infty$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : We assume the function $f$ attains its minimum at some point $\\mathbf{x}^\\star$:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}^\\star) \\leq f(\\mathbf{x}) \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^p.\n",
    "$$\n",
    "\n",
    "In a complete separation setting, for any scalar $\\alpha > 0$, we should have:\n",
    "\n",
    "$$\n",
    "b_i \\mathbf{a}_i^T (\\alpha \\mathbf{x}_0) = \\alpha (b_i \\mathbf{a}_i^T \\mathbf{x}_0) > 0.\n",
    "$$\n",
    "\n",
    "Goind back to our function $f$, and looking at $f(\\alpha \\mathbf{x}_0)$, we have :\n",
    "\n",
    "$$\n",
    "f(\\alpha \\mathbf{x}_0) = \\sum_{i=1}^n \\log(1 + \\exp(- b_i \\mathbf{a}_i^T (\\alpha \\mathbf{x}_0))).\n",
    "$$\n",
    "\n",
    "As $\\alpha \\to +\\infty$, $\\exp(-\\alpha (b_i \\mathbf{a}_i^T \\mathbf{x}_0)) \\to 0$. Therefore,\n",
    "\n",
    "$$\n",
    "f(\\alpha \\mathbf{x}_0) = \\sum_{i=1}^n \\log(1 + \\exp(-\\alpha (b_i \\mathbf{a}_i^T \\mathbf{x}_0))) \\to 0 \\quad \\text{as} \\quad \\alpha \\to +\\infty.\n",
    "$$\n",
    "\n",
    "If $f$ attains its minimum at $\\mathbf{x}^\\star$, then $f(\\mathbf{x}^\\star)$ must be a finite value. However, we have shown that $f(\\alpha \\mathbf{x}_0) \\to 0$ as $\\alpha \\to +\\infty$. Since $f$ is non-negative and convex, the only way for $f$ to approach 0 is if it never attains a minimum value greater than 0. Therefore, the assumption that $f$ attains its minimum leads to a contradiction. Hence, the function $f$ does not attain its minimum in a complete separation setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just shown convex functions do not always attain their infimum. So it is possible for the maximum-likelihood estimator $\\mathbf{x}^\\star_{ML}$ to not exist. We will resolve this issue by adding a regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we consider the function\n",
    "\n",
    "\\begin{equation*}\n",
    "\tf_\\mu(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2\n",
    "\\end{equation*}\n",
    "with $\\mu> 0$.\n",
    "\n",
    "__(f)__ (1 point) Show that the gradient of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "__Hint__: Lecture 3 shows you how to proceed with this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : Let's calculate the gradients for both terms:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\nabla \\log(1 + \\exp(- b_i \\mathbf{a}_i^T \\mathbf{x})).\n",
    "$$\n",
    "\n",
    "Using the chain rule, we have:\n",
    "\n",
    "$$\n",
    "\\nabla \\log(1 + \\exp(- b_i \\mathbf{a}_i^T \\mathbf{x})) = \\frac{1}{1 + \\exp(- b_i \\mathbf{a}_i^T \\mathbf{x})} \\cdot (-b_i \\mathbf{a}_i) \\cdot \\exp(- b_i \\mathbf{a}_i^T \\mathbf{x}) = -b_i \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i\n",
    "$$\n",
    "\n",
    "where we use the following : $\\frac{\\exp\\left(-b_i \\mathbf{a}_i^T \\mathbf{x}\\right)}{1 + \\exp\\left(-b_i \\mathbf{a}_i^T \\mathbf{x}\\right)} = \\sigma\\left(-b_i \\mathbf{a}_i^T \\mathbf{x}\\right)$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\nabla \\left( \\frac{\\mu}{2} \\|\\mathbf{x}\\|_2^2 \\right) = \\mu \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "Combining the gradients we get:\n",
    "\n",
    "$$\n",
    "\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x}))\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "\\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : The Hessian of $f(\\mathbf{x})$ is:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}) = \\sum_{i=1}^n \\nabla \\left( -b_i \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i \\right).\n",
    "$$\n",
    "\n",
    "Using the chain rule, we have:\n",
    "\n",
    "$$\n",
    "\\nabla \\left( -b_i \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i \\right) = -b_i \\mathbf{a}_i \\nabla \\left( \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\right)^T.\n",
    "$$\n",
    "\n",
    "The gradient of $\\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})$ is:\n",
    "\n",
    "$$\n",
    "\\nabla \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) = \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}))(-b_i \\mathbf{a}_i).\n",
    "$$\n",
    "\n",
    "Now we substitute this result back into the chain rule expression:\n",
    "\n",
    "$$\n",
    "\\nabla \\left( -b_i \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i \\right) = -b_i \\mathbf{a}_i \\left( \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}))(-b_i \\mathbf{a}_i) \\right)^T.\n",
    "$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$\n",
    "b_i^2 \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x}) (1 - \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T.\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, the Hessian of $f(\\mathbf{x})$ is:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}) = \\sum_{i=1}^n -b_i^2 \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T.\n",
    "$$\n",
    "\n",
    "The Hessian of the regularization term is:\n",
    "\n",
    "$$\n",
    "\\nabla^2 \\left( \\frac{\\mu}{2} \\|\\mathbf{x}\\|_2^2 \\right) = \\mu \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "Finally we get:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i^2 \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "but having $b_i^2$ is equal to 1:\n",
    "$$\n",
    "\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to observe that we can write the Hessian in a more compact form by defining the matrix\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{a}_1^T & \\rightarrow \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_2^T & \\rightarrow \\\\\n",
    "         &  \\ldots &  \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_n^T & \\rightarrow \\\\\n",
    "  \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "It is easy to see that we have\n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) =  \\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)\\mathbf{A}+ \\mu \\mathbf{I}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : From Lecture 3, a function $f_\\mu$ is $\\mu$-strongly convex if for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^p$, we have:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\langle \\nabla f(\\mathbf{x}), \\mathbf{y} - \\mathbf{x} \\rangle + \\frac{\\mu}{2} \\|\\mathbf{y} - \\mathbf{x}\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "Equivalently, $f_\\mu$ is $\\mu$-strongly convex if the Hessian $\\nabla^2 f_\\mu(\\mathbf{x})$ is positive definite with the smallest eigenvalue being at least $\\mu$, which can be written like $\\nabla^2 f(\\mathbf{x}) \\succeq \\mu \\mathbf{I}, \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^p$. To show that, we write that for any vector $\\mathbf{x}$, we have $\\mathbf{x}^T\\mathbf{A}^T \\mathbf{A}\\mathbf{x}= (\\mathbf{A}\\mathbf{x})^T(\\mathbf{A}\\mathbf{x}) = \\sum_{i=1}^{n} (\\mathbf{a}_i^T\\mathbf{x})^2 \\geq 0 $, which means that $\\mathbf{A}^T \\mathbf{A}$ is positive semidefinite.\n",
    "\n",
    "From the cell above, the term $\\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})) \\right) \\mathbf{A}$ is a positive semidefinite matrix the diagonal matrix has non-negative entries (since $\\sigma(u)(1 - \\sigma(u)) \\geq 0$ for all $u$).\n",
    "\n",
    "The term $\\mu \\mathbf{I}$ is a positive definite matrix with the smallest eigenvalue being $\\mu$. Therefore, the Hessian is positive definite with the smallest eigenvalue being at least $\\mu$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i)__ (3 points) Is it possible for a strongly convex function to not attain its minimum ? <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Justify your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: Strictly said the answer is yes. It is possible to find a function that is strongly convex while not attaining its minimum, when the function is lower semi-continous and its domain is a subset of  $\\mathbb{R^n}$. We did not come up with this, but found a post about it on a [forum](https://math.stackexchange.com/questions/2311335/existence-of-minimizer-for-strongly-convex-function-on-closed-convex-set).\n",
    "\n",
    "We will prove that the function $f: [0, \\infty] \\to \\mathbb{R}$,\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "1 & \\text{when } x = 0 \\\\\n",
    "x^2 & \\text{when } x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "is 2-strongly convex based on the definition of strong convexity, while not attaining its minimum.\n",
    "\n",
    "#### 1. Definition of Strong Convexity\n",
    "\n",
    "A function $f: \\mathcal{Q} \\to \\mathbb{R} \\cup \\{ + \\infty \\} $ , where $ \\mathcal{Q} \\subseteq \\mathbb{R}^p$, is said to be $\\mu$-strongly convex if and only if for all $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{Q}$ and $\\alpha \\in [0, 1]$, the following inequality holds:\n",
    "\n",
    "$$\n",
    "f(\\alpha \\mathbf{x} + (1 - \\alpha) \\mathbf{y}) \\leq \\alpha f(\\mathbf{x}) + (1 - \\alpha) f(\\mathbf{y}) - \\frac{\\mu}{2} \\alpha(1 - \\alpha) \\| \\mathbf{x} - \\mathbf{y} \\|_2^2\n",
    "$$\n",
    "\n",
    "We will verify that this inequality holds for the given function.\n",
    "\n",
    "#### 2. Check strong convexity for the given function:\n",
    "\n",
    "Case 1: $x = 0$ and $y > 0$\n",
    "- For $x = 0$, we have $f(0) = 1$\n",
    "- For $y > 0$, we have $f(y) = y^2$\n",
    "\n",
    "The inequality becomes:\n",
    "$$\n",
    "f(\\alpha \\cdot 0 + (1 - \\alpha) y) \\leq \\alpha f(0) + (1 - \\alpha) f(y) - \\frac{\\mu}{2} \\alpha(1 - \\alpha) \\| 0 - y \\|_2^2 \\\\\n",
    "$$\n",
    "\n",
    "This can futher be simplified,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Leftrightarrow (1 - \\alpha)^2 y^2 &\\leq \\alpha + (1 - \\alpha) y^2 - \\frac{\\mu}{2} \\alpha(1 - \\alpha) y^2 \\\\\n",
    "\\Leftrightarrow (1 - 2\\alpha + \\alpha^2) y^2 &\\leq \\alpha + (1 - \\alpha) y^2 - \\frac{\\mu}{2} (\\alpha - \\alpha^2) y^2 \\\\\n",
    "\\Leftrightarrow \\alpha^2 \\left( 1 - \\frac{\\mu}{2} \\right) y^2 &\\leq \\alpha + \\alpha \\left( 1 - \\frac{\\mu}{2} \\right) y^2\n",
    "\\end{align*}\n",
    "$$\n",
    "and since $\\alpha \\in [0, 1]$ this inequality always holds.\n",
    "\n",
    "\n",
    "Case 2: $x > 0$ and $y > 0$\n",
    "\n",
    "This just comes down to proving  that the function $x^2$ for $x > 0$ is 2-strongly convex. Which by the definition of strong convexity in lecture 3, slide 37 is the true. As,\n",
    "$$\n",
    "h(x) = f(x) - \\frac{\\mu}{2} \\|\\mathbf{x}\\|_2^2 = x^2 - \\frac{2}{2} x^2 = 0\n",
    "$$\n",
    "which is a convex function.\n",
    "\n",
    "\n",
    "#### 3. Conclusion\n",
    "\n",
    "From the cases above, we can conclude that the function $ f(x) $ is 2-strongly convex. However, the function does **not attain its minimum** because the minimum value would occur as $ x \\to 0^+ $, and at $ x = 0 $, $ f(0) = 1 $, which is greater than $ f(x) = x^2 $ for small $ x > 0 $. Hence, the function is strongly convex but does not attain its minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that $f_\\mu$ is smooth, i.e, $\\nabla f_\\mu$ is L-Lipschitz with respect to the Euclidean norm, with \n",
    "\\begin{equation}\n",
    "\tL = \\|A\\|^2_F + \\mu \\text{, where }\\|\\cdot\\|_F\\text{ denotes the Frobenius norm. }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point for all three questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : Consider the matrix $\\mathbf{a}_i \\mathbf{a}_i^T$, where $\\mathbf{a}_i \\in \\mathbb{R}^p$. This matrix is a rank-1 matrix because it is formed by the outer product of the vector $\\mathbf{a}_i$ with itself. The eigenvalues of a rank-1 matrix can be found the following way:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_i \\mathbf{a}_i^T \\mathbf{v} = \\lambda \\mathbf{v}.\n",
    "$$\n",
    "\n",
    "where $\\mathbf{v}$ is an eigenvector of $\\mathbf{a}_i \\mathbf{a}_i^T$ with eigenvalue $\\lambda$. Consider now the characteristic equation:\n",
    "\n",
    "$$\n",
    "\\det(\\mathbf{a}_i \\mathbf{a}_i^T - \\lambda \\mathbf{I}) = 0.\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{a}_i \\mathbf{a}_i^T$ is a rank-1 matrix, it has at most one non-zero eigenvalue. The other eigenvalues are zero. The non-zero eigenvalue can be found by considering the trace of the matrix:\n",
    "\n",
    "$$\n",
    "\\text{Tr}(\\mathbf{a}_i \\mathbf{a}_i^T) = \\mathbf{a}_i^T \\mathbf{a}_i = \\left\\| \\mathbf{a}_i \\right\\|_2^2.\n",
    "$$\n",
    "\n",
    "Therefore the non-zero eigenvalue must be $\\left\\| \\mathbf{a}_i \\right\\|_2^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-2)__ Using [2](#mjx-eqn-eq2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$. \n",
    "\n",
    "__Hint__: Recall that $\\lambda_{\\max}(\\cdot)$ verifies the triangle inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : From part (g), we have the Hessian of $f_\\mu$ expressed as:\n",
    "\n",
    "$$\n",
    "\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "We need to find an upper bound for $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x}))$. The following hint is given, for any matrices $\\mathbf{A}$ and $\\mathbf{B}$, we have:\n",
    "\n",
    "$$\n",
    "\\lambda_{\\max}(\\mathbf{A} + \\mathbf{B}) \\leq \\lambda_{\\max}(\\mathbf{A}) + \\lambda_{\\max}(\\mathbf{B}).\n",
    "$$\n",
    "\n",
    "Applying the triangle inequality to the Hessian of $f_\\mu$, we get:\n",
    "\n",
    "$$\n",
    "\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\lambda_{\\max}\\left( \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T \\right) + \\lambda_{\\max}(\\mu \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "\n",
    "Note that $\\sigma(u)(1 - \\sigma(u)) \\leq \\frac{1}{4}$ for all $u$, since $\\sigma(u) \\in [0, 1]$ : \n",
    "$$\n",
    "\\lambda_{\\max}\\left( \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T \\right) \\leq \\frac{1}{4} \\lambda_{\\max}(\\mathbf{a}_i \\mathbf{a}_i^T).\n",
    "$$\n",
    "\n",
    "From part (j-1), we know that $\\lambda_{\\max}(\\mathbf{a}_i \\mathbf{a}_i^T) = \\|\\mathbf{a}_i\\|_2^2$.\n",
    "\n",
    "$$\n",
    "\\lambda_{\\max}\\left( \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T \\mathbf{x})) \\mathbf{a}_i \\mathbf{a}_i^T \\right) \\leq \\frac{1}{4} \\|\\mathbf{a}_i\\|_2^2.\n",
    "$$\n",
    "\n",
    "For the term $\\mu \\mathbf{I}$, we have $\\lambda_{\\max}(\\mu \\mathbf{I}) = \\mu$.\n",
    "\n",
    "So we get:\n",
    "\n",
    "$$\n",
    "\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\frac{1}{4} \\|\\mathbf{a}_i\\|_2^2 + \\mu \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu..\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|A\\|_F^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER : $f_\\mu$ is $L$-smooth if $\\nabla^2 f_\\mu(\\mathbf{x})\\preceq L \\mathbf{I}$\n",
    "\n",
    "We can then take the previous result: $L = \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu = \\sum_{i=1}^{n} \\sum_{j=1}^{p} |\\mathbf{a}_{i,j}|^2 + \\mu$.\n",
    "\n",
    "$\\sum_{i=1}^{n} \\sum_{j=1}^{p} |\\mathbf{a}_{i,j}|^2$ is by definition the denotes the square of the Frobenius norm of A. We can then conclude that:\n",
    "\n",
    "$L = \\|A\\|_F^2 + \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(l)__ (2 point) To finalize, we introduce the Kullback-Leibler (KL) divergence. The KL divergence is a measure of how one probability distribution differs from a second, reference probability distribution. Along side the examples we saw in slide 18 of Lecture 1, the KL divergence is also a useful loss function to be used in learning frameworks.\n",
    "\n",
    "Write the definition of the Kullback-Leibler (KL) divergence between the true label distribution $q(b_i)$ and the model’s predicted distribution $p(b_i∣\\mathbf{a}_i)$ and show that minimizing the KL divergence between $q(b_i)$ and $p(b_i∣\\mathbf{a}_i)$ is equivalent to minimizing the negative log-likelihood derived in (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "\n",
    "$D_{KL}(q \\| p) = \\sum_{i} q(b_i) \\log\\left( \\frac{q(b_i)}{p(b_i \\mid \\mathbf{a}_i)} \\right)$ , with true label distribution $ q(b_i)$ and the model’s predicted distribution $p(b_i \\mid \\mathbf{a}_i)$\n",
    "\n",
    "With this definition, we can develop:\n",
    "\n",
    "$D_{KL}(q \\| p) = \\sum_{i} q(b_i) \\log(q(b_i)) - \\sum_{i} q(b_i) \\log(p(b_i \\mid \\mathbf{a}_i))$\n",
    "\n",
    "In the context of binary classification, the true label distribution $q(b_i)$ is typically a delta function centered at the true label $b_i$. This means:\n",
    "\n",
    "$$\n",
    "q(b_i) = \\begin{cases}\n",
    "1 & \\text{if } b_i \\text{ is the true label}, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The first term doesn't depend on the model's predicted distribution so we can remove it in the minimization problem formulation.\n",
    "\n",
    "$\\min D_{KL}(q \\| p) = - \\min \\sum_{i} q(b_i) \\log(p(b_i \\mid \\mathbf{a}_i))$\n",
    "\n",
    "From (a), we had:\n",
    "\n",
    "$f(\\mathbf{x}) = -\\log(\\mathbb{P}(b_1, \\dots, b_n | a_1, \\dots, a_n))  = - \\sum_{i=1}^n  \\log(p(b_i \\mid \\mathbf{a}_i))$\n",
    "\n",
    "Which is equivalent to minimizing the KL divergence between $q(b_i)$ and $p(b_i∣\\mathbf{a}_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From your work in this section, you have shown that the maximum likelihood estimator for logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_2^2$ regularizer. Consequently, the estimator for $\\mathbf{x}^\\natural$ we will use will be the solution of the smooth strongly convex problem,\n",
    "\\begin{equation}\n",
    "\t\\mathbf{x}^\\star=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2.\n",
    "\\tag{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) TA's will give you candy if you provide a complete proof."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
